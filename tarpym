#!/usr/bin/env python3
import argparse
import base64
import hashlib
import jinja2
import json
import os
import shutil
import six
import stat
import struct
import subprocess
import sys
import rpm
import tempfile


from collections import OrderedDict
from copy import deepcopy
from koji.util import DataWalker


def main():
    global args
    parser = argparse.ArgumentParser(prog='tarpym',
                                     description='convert rpms to and from exploded contents')
    parser.add_argument('-x', '--extract', action='store_true',
                        help='Extract rpm file to directory')
    parser.add_argument('-c', '--create', action='store_true',
                        help='Create rpm file from directory')
    parser.add_argument('-u', '--update', action='store_true',
                        help='Update extracted header data from payload contents')
    parser.add_argument('-f', '--filename', help='RPM filename')
    parser.add_argument('-d', '--directory', help='Directory of RPM contents')
    parser.add_argument('--no-payload', action='store_true', help='skip payload, for debugging')
    parser.add_argument('--cpio-payload', action='store_const', dest='payload', const='cpio',
                        default='rpmbuild', help='use cpio to construct the payload')
    parser.add_argument('--rpmbuild-payload', action='store_const', dest='payload',
                        const='rpmbuild', help='use rpmbuild to construct the payload')
    parser.add_argument('--raw-payload', action='store_true',
                        help='write payload.bin instead of expanding files')
    parser.add_argument('--cpio-extract', action='store_const', dest='extract', const='cpio',
                        default=None, help='use cpio to extract the payload')
    args = parser.parse_args()
    if args.extract:
        if not args.filename:
            parser.error('Please provide a filename for extraction')
        explode_rpm(args.filename, args.directory)
    elif args.create:
        if not args.directory:
            parser.error('Please provide a directory of rpm contents')
        create_rpm(args.directory, args.filename)
    elif args.update:
        if not args.directory:
            parser.error('Please provide a directory of rpm contents')
        update_header(args.directory)
    else:
        parser.error('No action specified')


def explode_rpm(rpmfile, directory=None):
    # parse lead
    lead = parse_lead(rpmfile)

    # parse header
    h = rip_rpm_hdr(rpmfile)
    rh = RawHeader(h, decode=True, is_sig=False)
    header = rh.to_dict()

    # parse signature header
    sighdr = rip_rpm_sighdr(rpmfile)
    rh = RawHeader(sighdr, decode=True, is_sig=True)
    sigheader = rh.to_dict()

    # determine directory to write to
    if directory is None:
        # calculate destination dir
        nvr = '%(name)s-%(version)s-%(release)s' % header['index']
        arch = 'src' if header['index'].get('sourcepackage') else header['index']['arch']
        base = f'{nvr}.{arch}'
        dest = base
        for i in range(2, 100):
            if os.path.exists(dest):
                dest = f'{base},{i}'
                continue
            break
    else:
        if os.path.exists(directory):
            error('Destination directory already exists')
        dest = directory
    os.mkdir(dest)

    # write lead + headers
    fn = os.path.join(dest, 'lead.json')
    with open(fn, 'wt') as fd:
        json.dump(lead, fd, indent=4)

    fn = os.path.join(dest, 'header.json')
    with open(fn, 'wt') as fd:
        json.dump(header, fd, indent=4)

    fn = os.path.join(dest, 'sig.json')
    with open(fn, 'wt') as fd:
        json.dump(sigheader, fd, indent=4)

    # write payload
    if not args.no_payload:
        if args.raw_payload:
            fn = os.path.join(dest, 'payload.bin')
            rip_payload(rpmfile, fn)
        else:
            pdir = os.path.join(dest, 'payload')
            os.mkdir(pdir)
            explode_payload(rpmfile, pdir)

    print(f'Expoded {rpmfile} to {dest}')


def create_rpm(directory, rpmfile=None):
    if not os.path.isdir(directory):
        error(f'No such directory: {directory}')

    # load header metadata
    fn = os.path.join(directory, 'header.json')
    with open(fn, 'rt') as fd:
        header = json.load(fd, object_hook=OrderedDict)

    # determine output file
    if rpmfile is None:
        # generate from header info
        nvr = '%(name)s-%(version)s-%(release)s' % header['index']
        arch = 'src' if header['index'].get('sourcepackage') else header['index']['arch']
        dest = f'{nvr}.{arch}.rpm'
    else:
        dest = rpmfile
    if os.path.exists(dest):
        error(f'RPM file already exists: {dest}')

    # write out payload first so we can update the header
    tmpfile = tempfile.TemporaryFile('w+b')
    p_info = {}
    if args.no_payload:
        # skip payload
        pass
    elif args.payload == 'rpmbuild':
        # use rpmbuild
        p_info = build_payload(tmpfile, header, directory)
    elif args.payload == 'cpio':
        # use cpio
        p_info = write_payload(tmpfile, header, directory)

    # make lead
    lead_fn = os.path.join(directory, 'lead.json')
    lead_data = None
    if os.path.exists(lead_fn):
        with open(lead_fn, 'rt') as fd:
            lead_data = json.load(fd, object_hook=OrderedDict)
            # actually order doesn't really matter going this way, but shouldn't hurt
    lead = make_lead(header, lead_data)

    # make header
    hdr = make_header(header)

    # make signature header
    psize = None
    if 'size' in p_info:
        # cpio case
        psize = p_info['size']
    elif 'sighdr' in p_info:
        # rpmbuild case
        _idx = p_info['sighdr']['index']
        if 'longarchivesize' in _idx:
            psize = _idx['longarchivesize']['value'][0]
            # TODO clean up access
        else:
            psize = p_info['sighdr']['index']['payloadsize'][0]
    print(f"Payload size {psize}")
    sig = make_sig(hdr, tmpfile, psize=psize)

    # finally, write out the rpm
    with open(dest, 'wb') as fd:
        fd.write(lead)
        fd.write(sig)
        fd.write(hdr)
        tmpfile.seek(0)  # payload
        shutil.copyfileobj(tmpfile, fd)

    print(f"Wrote: {dest}")


def update_header(directory):
    if not os.path.isdir(directory):
        error(f'No such directory: {directory}')
    pdir = os.path.join(directory, 'payload')
    if not os.path.isdir(pdir):
        error(f'Missing payload directory: {pdir}')

    # load header metadata
    header_fn = os.path.join(directory, 'header.json')
    with open(header_fn, 'rt') as fd:
        header = json.load(fd, object_hook=OrderedDict)

    changed = update_header_files(header, pdir)  # modifies header

    if not changed:
        print('No file changes detected')

    # backup
    for i in range(100):
        backup = f'{header_fn}.save.{i}'
        if not os.path.exists(backup):
            shutil.copyfile(header_fn, backup)
            break
    else:
        error('backup limit exhausted')

    # write new header
    with open(header_fn, 'wt') as fd:
        json.dump(header, fd, indent=4)


def update_header_files(header, pdir):
    # check payload files against header
    changed = False
    newfiles = []
    f_index = {}
    for finfo in header['files']:
        ipath = '%(dirname)s/%(basename)s' % finfo
        lpath = os.path.normpath(f'{pdir}/{ipath}')
        # TODO handle %ghost files
        try:
            st = os.lstat(lpath)
        except FileNotFoundError:
            print(f'Dropping file {ipath}')
            continue

        f_index[lpath] = finfo

        if stat.S_ISDIR(st.st_mode):
            # TODO handle directories
            newfiles.append(finfo)
            continue

        if stat.S_ISLNK(st.st_mode):
            # TODO handle symlinks
            newfiles.append(finfo)
            continue

        # size
        if st.st_size != finfo['size']:
            print(f'Adjust size for {ipath} to {st.st_size}')
            changed = True
            finfo['size'] = st.st_size

        # TODO abbreviated check option

        # checksum
        # XXX don't hardcode sha256
        csum = hashlib.sha256()
        with open(lpath, 'rb') as fp:
            while True:
                chunk = fp.read(8192)
                if not chunk:
                    break
                csum.update(chunk)

        # TODO handle symlinks

        digest = csum.hexdigest()
        if digest != finfo['md5']:
            # the primary header name is a misnomer
            print(f'Adjust checksum for {ipath} to {digest}')
            changed = True
            finfo['md5'] = digest

        newfiles.append(finfo)

    # find new files
    for dirpath, dirnames, filenames in os.walk(pdir):
        for fn in filenames:
            lpath = os.path.normpath(f'{dirpath}/{fn}')
            if lpath in f_index:
                continue
            print(f'New file: {lpath}')
            finfo = new_file_entry(lpath, pdir)
            newfiles.append(finfo)
            changed = True
        if not dirnames and not filenames:
            lpath = os.path.normpath(dirpath)
            if lpath in f_index:
                continue
            print(f'New empty directory: {lpath}')
            finfo = new_file_entry(lpath, pdir)
            newfiles.append(finfo)
            changed = True

    header['files'] = newfiles

    # TODO also adjust header['index']['size']

    return changed


def new_file_entry(lpath, pdir):
    # FIXME incomplete
    print('WARNING: support for new files is incomplete')
    st = os.lstat(lpath)
    dirpath = os.path.dirname(lpath)  # also pass in?
    finfo = {
        'basename': os.path.basename(lpath),
        'dirname': os.path.relpath(dirpath, pdir),
        'size': st.st_size,
        # TODO lots more
    }

    # checksum
    # XXX don't hardcode sha256
    csum = hashlib.sha256()
    with open(lpath, 'rb') as fp:
        while True:
            chunk = fp.read(8192)
            if not chunk:
                break
            csum.update(chunk)

    # TODO handle symlinks
    # TODO handle directories

    digest = csum.hexdigest()
    finfo['md5'] = csum.hexdigest()
    # the primary header name is a misnomer

    return finfo


# Data for rpmlead struct
# See Maximum RPM, Appendix A, RPM File Format
# https://ftp.osuosl.org/pub/rpm/max-rpm/s1-rpm-file-format-rpm-file-format.html
# "Package files are written to disk in network byte order
LEAD_MAP = (
    ('magic', '4s'),            # unsigned char magic[4];
    ('major', 'B'),
    ('minor', 'B'),             # unsigned char major, minor;
    ('type', 'h'),              # short type;
    ('archnum', 'h'),           # short archnum;
    ('name', '66s'),            # char name[66];
    ('osnum', 'h'),             # short osnum;
    ('signature_type', 'h'),    # short signature_type;
    ('reserved', '16s')         # char reserved[16];
)
LEAD_FMT = '!' + ''.join([s for (k, s) in LEAD_MAP])  # network byte order


def parse_lead(src):
    """Parse the lead out of an rpm"""
    fmt = LEAD_FMT
    lead_size = 96
    assert struct.calcsize(fmt) == lead_size
    with open(src, 'rb') as fp:
        lead_bin = fp.read(lead_size)
    lead = OrderedDict()
    data = struct.unpack(fmt, lead_bin)
    for (key, _s), value in zip(LEAD_MAP, data):
        lead[key] = value

    # sanity checks
    if lead['magic'] != RPM_LEAD_MAGIC:
        raise ValueError("Invalid rpm lead: bad magic: %(magic)r" % lead)
    if lead['type'] not in (0, 1):
        print('Warning: unknown type in lead %(type)r' % lead)
    if lead['reserved'] != b'\0' * 16:
        print('Warning: unexpected reserved content in lead %(reserved)r' % lead)

    # convert values to be json friendly
    lead['name'] = lead['name'].rstrip(b'\0').decode('utf8', errors='surrogateescape')
    lead['magic'] = base64.b64encode(lead['magic']).decode('ascii')
    lead['reserved'] = base64.b64encode(lead['reserved']).decode('ascii')

    return lead


def encode_lead(lead):
    """Encode any decoded data in lead dict"""
    # see the corresponding conversions in parse_lead
    lead = lead.copy()
    if 'name' in lead:
        name = lead['name']
        if isinstance(name, str):
            lead['name'] = name.encode(errors='surrogatepass')
    for key in 'magic', 'reserved':
        if key in lead:
            value = lead[key]
            if isinstance(value, str):
                lead[key] = base64.b64decode(value)
    return lead


def make_lead(header, data=None):
    """Generate a lead blob"""
    if data is None:
        data = {}
    else:
        data = encode_lead(data)

    magic = b'\xED\xAB\xEE\xDB'
    if 'magic' in data and data['magic'] != magic:
        print('Warning: invalid lead magic specified: %(magic)r' % data)
        # but do what the user says

    # "Versions of RPM later than 2.1 create version 3.0 package files
    data.setdefault('major', 3)
    data.setdefault('minor', 0)

    if data.get('type') is None:
        # binary 0000, src 0001
        data['type'] = 1 if header['index'].get('sourcepackage') else 0

    data.setdefault('archnum', 255)

    # handle the name
    name = data.get('name')
    if name is None:
        # generate from header
        name = '%(name)s-%(version)s-%(release)s' % header['index']
        name = name.encode(errors='surrogatepass')
        name = name[:66]
    elif len(name) > 66:
        print('Warning: lead name is too long. Truncating')
        name = name[:66]
    # pad regardless
    pad = b'\x00' * (66 - len(name))
    data['name'] = name + pad

    data.setdefault('osnum', 1)     # 0001 = linux
    data.setdefault('sigtype', 5)
    data.setdefault('reserved', b'\x00' * 16)

    values = [data[k] for (k, s) in LEAD_MAP]
    lead = struct.pack(LEAD_FMT, *values)
    assert len(lead) == 96
    return lead


def make_header(header):
    rh = RawHeader(header, is_sig=False)
    return rh.header


def write_payload(tmpfile, header, directory):
    # TODO handle "stripped down" cpio used for >4G
    pdir = f'{directory}/payload'

    # check that we have all the files
    flist = []
    for finfo in header['files']:
        relpath = './%s/%s' % (finfo['dirname'].lstrip('/'), finfo['basename'])
        fn = f'{pdir}/{relpath}'
        if not os.path.exists(fn):
            raise Exception(f'Missing payload file: {fn}')
        flist.append(relpath)

    workdir = tempfile.mkdtemp()
    try:
        # write the file list to a temporary file
        # we'll pass this list to our cpio command
        tmplist = f'{workdir}/flist'
        with open(tmplist, 'wt') as fp:
            for relpath in flist:
                fp.write(relpath)
                fp.write('\0')  # null separated

        # generate compressed payload
        cmd1 = ['cpio', '-o', '--null', '-H', 'crc', '--no-absolute-filenames',
                '--ignore-devno', '--device-independent', '-v']
        cmd2 = ['zstd', '-19']
        raw = f'{workdir}/raw_payload'
        with open(tmplist, 'rb') as inp:
            with open(raw, 'wb') as outp:
                p1 = subprocess.Popen(cmd1, cwd=pdir, stdin=inp, stdout=outp)
                assert p1.wait() == 0
        with open(raw, 'rb') as inp:
            p2 = subprocess.Popen(cmd2, stdin=inp, stdout=tmpfile)
            assert p2.wait() == 0

        st = os.stat(raw)
        psize = st.st_size

    finally:
        shutil.rmtree(workdir)

    # XXX make this more flexible
    header['index']['payloadformat'] = 'cpio'
    # header['index']['payloadcompressor'] = 'gzip'
    header['index']['payloadcompressor'] = 'zstd'
    header['index']['payloadflags'] = '19'
    if 'payloaddigestalt' in header['index']:
        # XXX skipping the uncompressed checksum for now
        del header['index']['payloaddigestalt']  # XXX
    header['index']['payloaddigestalgo'] = {'value': [8], 'dtype': 4}  # sha256

    # calculate and record payload digest
    tmpfile.seek(0)
    csum = hashlib.sha256()
    while True:
        chunk = tmpfile.read(8192)
        if not chunk:
            break
        csum.update(chunk)
    header['index']['payloaddigest'] = {'value': [csum.hexdigest()], 'dtype': 8}

    return {'size': psize}


def build_payload(tmpfile, header, directory):

    payload_dir = f'{directory}/payload'
    pdirs = os.listdir(payload_dir)

    # define a tempdir for rpmbuild
    workdir = tempfile.mkdtemp()

    # make the file list
    flist = []
    dlist = []
    for finfo in header.get('files', []):
        # %ghost files are not in the payload
        if finfo['flags'] & rpm.RPMFILE_GHOST:
            # TODO we should pass it through as %ghost in case rpm changes
            continue
        if stat.S_ISDIR(finfo['mode']):
            dlist.append('%(dirname)s%(basename)s' % finfo)
        #elif stat.S_ISREG(finfo['mode']):
        else:
            flist.append('%(dirname)s%(basename)s' % finfo)
        # TODO do I need to distinguish more types?

    # expand the spec template
    env = jinja2.Environment(loader=jinja2.DictLoader(templates), autoescape=False)
    template = env.get_template('payload.spec')
    contents = template.render(files=flist, dirs=dlist, pdirs=pdirs)
    spec = f'{workdir}/payload.spec'
    with open(spec, 'wt') as fp:
        fp.write(contents)

    # run rpmbuild
    ppath = os.path.abspath(payload_dir)
    cmd = ['rpmbuild',
           '--define', f'_topdir {workdir}',
           '--define', f'_sourcedir {workdir}',
           '--define', f'_builddir {workdir}',
           '--define', f'_srcrpmdir {workdir}',
           '--define', f'_rpmdir {workdir}',
           '--define', f'payloaddir {ppath}',
           '-bb', spec]
    p = subprocess.run(cmd, capture_output=True)
    if p.returncode != 0:
        sys.stderr.write(p.stderr.decode('utf8'))
        print(f'workdir={workdir}')
        error('failed to build payload')
    p.check_returncode()

    # now rip the payload out of this rpm
    tmp_rpm = f'{workdir}/noarch/payload-1-1.noarch.rpm'
    # skip past the headers
    (start, size) = find_rpm_sighdr(tmp_rpm)
    start += size
    size = rpm_hdr_size(tmp_rpm, start)
    start += size
    with open(tmp_rpm, 'rb') as inp:
        inp.seek(start)
        # TODO verify checksum
        shutil.copyfileobj(inp, tmpfile)

    # copy payload fields from the header
    h = rip_rpm_hdr(tmp_rpm)
    rh = RawHeader(h, decode=True, is_sig=False)
    fakehdr = rh.to_dict()
    for key in fakehdr['index']:
        if key.startswith('payload'):
            header['index'][key] = fakehdr['index'][key]
    for key in header['index']:
        if key.startswith('payload') and key not in fakehdr['index']:
            print(f'Warning: dropping {key} field')
            del header['index'][key]

    # also fetch the sighdr for later code to use
    h = rip_rpm_sighdr(tmp_rpm)
    rh = RawHeader(h, decode=True, is_sig=True)
    fakesighdr = rh.to_dict()

    shutil.rmtree(workdir)

    return {'hdr': fakehdr, 'sighdr': fakesighdr}


# This is a jinja2 template for a spec file
TEMPLATE = '''
Name:           payload
Version:        1
Release:        1
Summary:        fake spec to get a payload
License:        LGPL
BuildArch:      noarch
AutoReqProv:    no

# we don't want any modifications to our files
%define __os_install_post %{nil}
%define __arch_install_post %{nil}
# we're always noarch
%define _binaries_in_noarch_packages_terminate_build 0

%description
We're only building an rpm to extract the payload, so only the files matter
Also, this is a terrible hack.

%install
# payloaddir macro is passed in, so we just copy the files
{# note mix of rpm and jinja macros #}
{%- for pdir in pdirs %}
cp -ai "%{payloaddir}/{{ pdir }}" %{buildroot}
{%- endfor %}

%files
{%- for path in files %}
"{{ path }}"
{%- endfor %}
{%- for path in dirs %}
%dir "{{ path }}"
{%- endfor %}

%changelog
* Wed Apr 23 2025 A Person <person at example.com> - 1.0-1
- rpm is happier with a changelog
'''
# FIXME properly escape quoted paths in spec template


templates = {'payload.spec': TEMPLATE}


def make_sig(hdr, tmpfile, psize=None):
    # find sigmd5 and payloadsize
    md5sum = hashlib.md5(hdr)
    tmpfile.seek(0)
    while True:
        chunk = tmpfile.read(8192)
        if not chunk:
            break
        md5sum.update(chunk)
    actual_psize = tmpfile.tell()
    if psize is not None:
        payloadsize = psize
    else:
        payloadsize = actual_psize
        # XXX this is the compressed size, we want the uncompressed size
        # RPMSIGTAG_PAYLOADSIZE = 1007,/*!< internal uncompressed payload size (32bit) in bytes. */

    index = {
        "sha1header": hashlib.sha1(hdr).hexdigest(),
        "sha256header": hashlib.sha256(hdr).hexdigest(),
        # RPMSIGTAG_SIZE»·····= 1000,»/*!< internal Header+Payload size (32bit) in bytes. */
        "md5": {
            "value": encode_bytes(md5sum.digest()),
            "dtype": 7
        },
        "reservedspace": {
            # default __gpg_reserved_space (4096) plus 32
            # see rpmGenerateSignature() in rpm source
            "value": encode_bytes(b'\0' * 4128),
            "dtype": 7
        },
    }

    if payloadsize > 4 * 1024 ** 3:
        index['longarchivesize'] = {'value': [payloadsize], 'dtype': 5}
        index['longsigsize'] = {'value': [len(hdr) + actual_psize], 'dtype': 5}
        # TODO make this less awkward
    else:
        index['payloadsize'] = [payloadsize]
        index['size'] = [len(hdr) + actual_psize]

    # RawHeader will add region tag for us

    # assemble the data into a header
    data = {'version': 1, 'index': index}
    rh = RawHeader(data, is_sig=True)
    return rh.header


def error(msg):
    print(msg)
    sys.exit(1)


def rip_payload(rpmfile, fn):
    """Simply write payload blob to a file"""
    # TODO check payload size against header
    # TODO avoid rescanning the headers
    sig_start, sigsize = find_rpm_sighdr(rpmfile)
    ofs = sig_start + sigsize
    hdrsize = rpm_hdr_size(rpmfile, ofs)
    with open(rpmfile, 'rb') as inf:
        # skip the headers
        inf.seek(ofs + hdrsize)
        with open(fn, 'wb') as outf:
            while True:
                chunk = inf.read(81920)
                if not chunk:
                    break
                outf.write(chunk)


def explode_payload(fn, dest):
    """Write files from rpm payload to destination dir"""
    r2a = shutil.which('rpm2archive')
    # TODO handle py < 3.3 case
    if not r2a or args.extract == 'cpio':
        return explode_payload_cpio(fn, dest)
    cmd1 = [r2a, fn]
    cmd2 = ['tar', '-xz']
    p1 = subprocess.Popen(cmd1, stdout=subprocess.PIPE)
    p2 = subprocess.Popen(cmd2, cwd=dest, stdin=p1.stdout)
    p1.stdout.close()
    p2.communicate()
    rv1 = p1.wait()
    if rv1 != 0:
        print(f'Warning: rpm2archive returned code {rv1}')
    assert p2.wait() == 0
    # fix modes so we can read/write
    cmd3 = ['chmod', '-R', 'u+rw', dest]
    p3 = subprocess.Popen(cmd3)
    assert p3.wait() == 0


def explode_payload_cpio(fn, dest):
    cmd1 = ['rpm2cpio', fn]
    cmd2 = ['cpio', '-iud']
    p1 = subprocess.Popen(cmd1, stdout=subprocess.PIPE)
    p2 = subprocess.Popen(cmd2, cwd=dest, stdin=p1.stdout)
    p1.stdout.close()
    p2.communicate()
    rv1 = p1.wait()
    if rv1 != 0:
        print(f'Warning: rpm2cpio returned code {rv1}')
    assert p2.wait() == 0
    # fix modes so we can read/write
    cmd3 = ['chmod', '-R', 'u+rw', dest]
    p3 = subprocess.Popen(cmd3)
    assert p3.wait() == 0



# Constants #

RPM_LEAD_MAGIC = six.b('\xed\xab\xee\xdb')
RPM_HEADER_MAGIC = six.b('\x8e\xad\xe8')
RPM_TAG_HEADERSIGNATURES = 62
RPM_TAG_FILEDIGESTALGO = 5011
RPM_SIGTAG_DSA = 267
RPM_SIGTAG_RSA = 268
RPM_SIGTAG_PGP = 1002
RPM_SIGTAG_MD5 = 1004
RPM_SIGTAG_GPG = 1005

RPM_FILEDIGESTALGO_IDS = {
    # Taken from RFC 4880
    # A missing algo ID means md5
    None: 'MD5',
    1: 'MD5',
    2: 'SHA1',
    3: 'RIPEMD160',
    8: 'SHA256',
    9: 'SHA384',
    10: 'SHA512',
    11: 'SHA224'
}

# rpm 4.12 introduces optional deps, but they can also be backported in some
# rpm installations. So, we need to check their real support, not only rpm
# version.
SUPPORTED_OPT_DEP_HDRS = {}
for h in (
        'SUGGESTNAME', 'SUGGESTVERSION', 'SUGGESTFLAGS',
        'ENHANCENAME', 'ENHANCEVERSION', 'ENHANCEFLAGS',
        'SUPPLEMENTNAME', 'SUPPLEMENTVERSION', 'SUPPLEMENTFLAGS',
        'RECOMMENDNAME', 'RECOMMENDVERSION', 'RECOMMENDFLAGS'):
    SUPPORTED_OPT_DEP_HDRS[h] = hasattr(rpm, 'RPMTAG_%s' % h)


# dependency types
DEP_REQUIRE = 0
DEP_PROVIDE = 1
DEP_OBSOLETE = 2
DEP_CONFLICT = 3
DEP_SUGGEST = 4
DEP_ENHANCE = 5
DEP_SUPPLEMENT = 6
DEP_RECOMMEND = 7

# dependency flags
RPMSENSE_LESS = 2
RPMSENSE_GREATER = 4
RPMSENSE_EQUAL = 8


def ensuredir(directory):
    """Create directory, if necessary.

    :param str directory: path of the directory

    :returns: str: normalized directory path

    :raises OSError: If argument already exists and is not a directory, or
                     error occurs from underlying `os.mkdir`.
    """
    directory = os.path.normpath(directory)
    if os.path.exists(directory):
        if not os.path.isdir(directory):
            raise OSError("Not a directory: %s" % directory)
    else:
        head, tail = os.path.split(directory)
        if not tail and head == directory:
            # can only happen if directory == '/' or equivalent
            # (which obviously should not happen)
            raise OSError("root directory missing? %s" % directory)
        if head:
            ensuredir(head)
        # note: if head is blank, then we've reached the top of a relative path
        try:
            os.mkdir(directory)
        except OSError:
            # do not thrown when dir already exists (could happen in a race)
            if not os.path.isdir(directory):
                # something else must have gone wrong
                raise
    return directory


def multibyte(data):
    """Convert a list of bytes to an integer (network byte order)"""
    sum = 0
    n = len(data)
    for i in range(n):
        sum += data[i] << (8 * (n - i - 1))
    return sum


def find_rpm_sighdr(path):
    """Finds the offset and length of the signature header."""
    # see Maximum RPM Appendix A: Format of the RPM File

    # The lead is a fixed sized section (96 bytes) that is mostly obsolete
    sig_start = 96
    sigsize = rpm_hdr_size(path, sig_start, pad=True)
    # "As of RPM 2.1 ... the Signature section is padded to a multiple of 8 bytes"
    return (sig_start, sigsize)


def rpm_hdr_size(f, ofs=None, pad=False):
    """Returns the length (in bytes) of the rpm header

    f = filename or file object
    ofs = offset of the header
    """
    if isinstance(f, six.string_types):
        fo = open(f, 'rb')
    else:
        fo = f
    if ofs is not None:
        fo.seek(ofs, 0)
    magic = fo.read(3)
    if magic != RPM_HEADER_MAGIC:
        raise ValueError("Invalid rpm: bad magic: %r" % magic)

    # skip past section magic and such
    #   (3 bytes magic, 1 byte version number, 4 bytes reserved)
    fo.seek(ofs + 8, 0)

    # now read two 4-byte integers which tell us
    #  - # of index entries
    #  - bytes of data in header
    data = [_ord(x) for x in fo.read(8)]
    il = multibyte(data[0:4])
    dl = multibyte(data[4:8])

    # this is what the section data says the size should be
    hdrsize = 8 + 16 * il + dl

    if pad:
        # signature headers are padded to a multiple of 8 bytes
        hdrsize = hdrsize + (8 - (hdrsize % 8)) % 8

    # add eight bytes for section header
    hdrsize = hdrsize + 8

    if isinstance(f, six.string_types):
        fo.close()
    return hdrsize


class RawHeader(object):

    # see Maximum RPM Appendix A: Format of the RPM File

    def __init__(self, data, decode=False, is_sig=None):
        self._tag_map = None
        self.decode = decode
        self._is_sig = is_sig  # None means auto-detect
        if isinstance(data, dict):
            self._orig = data
            data = self.make_header(data)
        if data[0:3] != RPM_HEADER_MAGIC:
            raise ValueError("Invalid rpm header: bad magic: %r" % (data[0:3],))
        self.header = data
        self._index()

    @property
    def is_sig(self):
        if self._is_sig is None:
            # detect whether this is a signature header
            if hasattr(self, 'index'):
                self._is_sig = bool(self.get(RPM_TAG_HEADERSIGNATURES))
            elif hasattr(self, '_orig'):
                self._is_sig = bool(self._orig.get('headersignatures'))
        return self._is_sig

    def get_tag_map(self):
        # map some rpmtag codes
        tags = {}
        if rpm:
            for name, code in six.iteritems(rpm.__dict__):
                if name.startswith('RPMTAG_') and isinstance(code, int):
                    tags[code] = name[7:].lower()
        else:
            print("rpm's python bindings are not installed. Unable to convert tag codes")
        if self.is_sig:
            # signature headers have a few different values
            # the SIGTAG_* values are not exposed in the python api
            # see rpmtag.h
            tags[1000] = 'size'
            tags[1001] = 'lemd5_1'
            tags[1002] = 'pgp'
            tags[1003] = 'lemd5_2'
            tags[1004] = 'md5'
            tags[1005] = 'gpg'
            tags[1006] = 'pgp5'
            tags[1007] = 'payloadsize'
            tags[1008] = 'reservedspace'
        return tags

    @property
    def tag_map(self):
        if self._tag_map is None:
            self._tag_map = self.get_tag_map()
        return self._tag_map

    def get_tag_name(self, code):
        if code in self.tag_map:
            return self.tag_map[code]
        else:
            return "RPM Tag %i" % code

    def version(self):
        # fourth byte is the version
        return _ord(self.header[3])

    def _index(self):
        # read two 4-byte integers which tell us
        #  - # of index entries  (each 16 bytes long)
        #  - bytes of data in header
        data = [_ord(x) for x in self.header[8:12]]
        il = multibyte(data[:4])
        dl = multibyte(data[4:8])

        # read the index (starts at offset 16)
        order = []
        for i in range(il):
            entry = []
            for j in range(4):
                ofs = 16 + i * 16 + j * 4
                data = [_ord(x) for x in self.header[ofs:ofs + 4]]
                entry.append(multibyte(data))

            # print("Tag: %d, Type: %d, Offset: %x, Count: %d" % tuple(entry))
            order.append(entry)
        self.datalen = dl
        self._order = order
        self.index = dict([(entry[0], entry) for entry in order])

    def to_dict(self, sig=None, decode=None):
        """Return an ordered dictionary representing the header contents"""
        result = OrderedDict()

        # first 3 bytes are magic
        # 4th byte is version
        result['version'] = self.version()

        # next 4 bytes are reserved, i.e. data[4:7]

        # at this point the remainder of the header is index
        # these will be in same order that the header index has them
        idx = result['index'] = OrderedDict()
        order = list(self._order)  # copy
        order.sort(key=lambda x: x[2])  # sort by offset in store rather than position in index
        for rpmtag, dtype, offset, count in order:
            key = self.get_tag_name(rpmtag)
            value = self.get(rpmtag, decode=decode)
            if dtype not in (4, 6, 9):
                # record explicit dtype
                # otherwise we can't distinguish later
                value = OrderedDict(
                    value=value,
                    dtype=dtype,
                )
            idx[key] = value

        # group file data for convenience and readability
        self._group_headers(result)

        # fix encoding
        result = encode_bytes_recurse(result)

        return result

    def _group_headers(self, result):
        # modifies result
        idx = result['index']

        # group file data for convenience and readability
        if 'basenames' in idx:
            basenames = idx['basenames']
            if isinstance(basenames, dict):
                basenames = basenames['value']
            files = result['files'] = [OrderedDict(basename=v) for v in basenames]
            del idx['basenames']
            for key, subkey in self.filesmap:
                if key not in idx:
                    continue
                previous = idx[key]
                if isinstance(previous, dict):
                    dtype = previous['dtype']
                    previous = previous['value']
                    # TODO handle dtype
                if len(previous) != len(files):
                    # XXX should never happen
                    print(f'File header length mismatch for {key} '
                          f'{len(previous)} != {len(files)}')
                    continue
                for i, finfo in enumerate(files):
                    finfo[subkey] = previous[i]
                del idx[key]

            # dereference dirnames
            dirnames = idx.get('dirnames', None)
            if isinstance(dirnames, dict):
                dirnames = dirnames['value']
                # TODO dtypes
            if dirnames and 'dirindexes' in idx:
                dirindex = idx['dirindexes']
                if isinstance(dirindex, dict):
                    dirindex = dirindex['value']
                if len(dirindex) != len(files):
                    # XXX should never happen
                    print(f'Directory index length mismatch: '
                          f'{len(dirindex)} != {len(files)}')
                else:
                    for i, finfo in enumerate(files):
                        dirnum = dirindex[i]
                        if dirnum >= len(dirnames):
                            raise ValueError(f'Directory index value out of bounds: {dirnum}')
                        finfo['dirname'] = dirnames[dirnum]
                del idx['dirnames']
                del idx['dirindexes']

            # defererence file classes
            # Fileclass           | 1141 | int32 array  | Index into Classdict
            classes = idx.get('classdict')
            if isinstance(classes, dict):
                classes = classes['value']
                # TODO dtypes?
            if classes:
                for finfo in files:
                    if 'class' in finfo:
                        cls_idx = finfo['class']
                        if cls_idx >= len(classes):
                            raise ValueError(f'File class index out of bounds: {cls_idx}')
                        finfo['class'] = classes[cls_idx]
                    else:
                        # TODO should we be stricter here?
                        print(f'Warning: no class info for file {finfo}')
                del idx['classdict']

        # the other groupings are simpler
        for group, gmap in self.zipmap:
            glist = None
            for key, subkey in gmap:
                if key not in idx:
                    glist = None
                    break
                previous = idx[key]
                if isinstance(previous, dict):
                    dtype = previous['dtype']
                    previous = previous['value']
                    # TODO handle dtype
                if glist is None:
                    glist = [OrderedDict({subkey: v}) for v in previous]
                elif len(previous) != len(glist):
                    # XXX should never happen
                    print(f'{group} header length mismatch for {key} '
                          f'{len(previous)} != {len(glist)}')
                    continue
                else:
                    for i, ginfo in enumerate(glist):
                        ginfo[subkey] = previous[i]

            if glist is not None:
                result[group] = glist
                for key, subkey in gmap:
                    del idx[key]

    def _ungroup_headers(self, data):
        # ungroup file data
        # returns an updated dict
        data = deepcopy(data)
        idx = data['index']

        self._ungroup_files(data)

        for group, gmap in self.zipmap:
            if group not in data:
                continue

            gheaders = {}
            for ginfo in data[group]:
                for key, subkey in gmap:
                    gheaders.setdefault(key, []).append(ginfo[subkey])

            del data[group]
            idx.update(gheaders)

        return data

    def _ungroup_files(self, data):
        # modifies data in place
        if 'files' not in data:
            return

        idx = data['index']
        if 'basenames' in idx:
            raise ValueError('Found duplicate files data')

        fileheaders = {}

        classmap = OrderedDict()
        for finfo in data['files']:
            # first pass gathers the file classes in order
            if 'class' in finfo and isinstance(finfo['class'], str):
                if finfo['class'] not in classmap:
                    classmap[finfo['class']] = len(classmap)  # so, reverse zero index
        for finfo in data['files']:
            if 'class' in finfo and finfo['class'] in classmap:
                finfo['class'] = classmap[finfo['class']]
        fileheaders['classdict'] = list(classmap.keys())

        basenames = fileheaders['basenames'] = []
        dirmap = OrderedDict()
        for finfo in data['files']:
            basenames.append(finfo['basename'])
            if finfo['dirname'] not in dirmap:
                # first time seeing, enumerate it in map
                dirmap[finfo['dirname']] = len(dirmap)
            dirindex = dirmap[finfo['dirname']]
            fileheaders.setdefault('dirindexes', []).append(dirindex)
            # i.e. if we haven't seen the dir yet, map it to its enumeration
            for key, subkey in self.filesmap:
                if subkey not in finfo:
                    # TODO defaults for missing keys?
                    continue
                fileheaders.setdefault(key, []).append(finfo[subkey])
                # TODO handle dtypes
        fileheaders['dirnames'] = list(dirmap)

        del data['files']
        idx.update(fileheaders)

    filesmap = [
        ('filesizes', 'size'),
        ('filemodes', 'mode'),
        ('filerdevs', 'rdev'),
        ('filemtimes', 'mtime'),
        ('filemd5s', 'md5'),
        ('filelinktos', 'linkto'),
        ('fileflags', 'flags'),
        ('fileusername', 'username'),
        ('filegroupname', 'groupname'),
        ('fileverifyflags', 'verifyflags'),
        ('filedevices', 'device'),
        ('fileinodes', 'inode'),
        ('filelangs', 'lang'),
        # ('dirindexes', 'dirindexes'),
        # ('basenames', 'basenames'),
        # ('*dirnames', '*dirnames'),
        ('filecolors', 'color'),
        ('fileclass', 'class'),
        ('filedependsx', 'dependsx'),
        ('filedependsn', 'dependsn'),
        ('longfilesizes', 'longsize'),
    ]

    typemap = {
        'filesizes': 4,
        'longfilesizes': 5,
        'filemodes': 3,
        'filerdevs': 3,
        'filemtimes': 4,
        'filemd5s': 8,
        'filelinktos': 8,
        'fileflags': 4,
        'fileusername': 8,
        'filegroupname': 8,
        'fileverifyflags': 4,
        'filedevices': 4,
        'fileinodes': 4,
        'filelangs': 8,
        'dirindexes': 4,
        'basenames': 8,
        '*dirnames': 8,
        'filecolors': 4,
        'fileclass': 4,
        'filedependsx': 4,
        'filedependsn': 4,
        'dirnames': 8,
        'changelogtime': 4,
        'changelogname': 8,
        'changelogtext': 8,
        'provides': 8,
        'provideflags': 4,
        'provideversion': 8,
        'requires': 8,
        'requireflags': 4,
        'requireversion': 8,
        'obsoletes': 8,
        'obsoleteflags': 4,
        'obsoleteversion': 8,
        'conflicts': 8,
        'conflictflags': 4,
        'conflictversion': 8,
        'classdict': 8,
    }

    # headers to "zip"
    zipmap = (
        ('changelog', (
            ('changelogtime', 'time'),
            ('changelogname', 'name'),
            ('changelogtext', 'text'),
         )),
        ('provides', (
            ('provides', 'value'),
            ('provideflags', 'flags'),
            ('provideversion', 'version'),
         )),
        ('requires', (
            ('requires', 'value'),
            ('requireflags', 'flags'),
            ('requireversion', 'version'),
         )),
        ('obsoletes', (
            ('obsoletes', 'value'),
            ('obsoleteflags', 'flags'),
            ('obsoleteversion', 'version'),
         )),
        ('conflicts', (
            ('conflicts', 'value'),
            ('conflictflags', 'flags'),
            ('conflictversion', 'version'),
         )),
    )

    def make_header(self, data):
        """Create binary header data from a dictionary"""
        data = self._ungroup_headers(data)
        version = data['version']
        index = data['index']

        # reverse tag_map
        tagmap = self.tag_map
        keymap = {tagmap[k]: k for k in tagmap}
        if self.is_sig:
            keymap['size'] = 1000  # XXX workaround

        # make sure headerimmutable is at the end
        if 'headerimmutable' in index:
            index.move_to_end('headerimmutable')

        idx = []
        store = []
        ofs = 0
        int_fmt = {1: 'B', 2: 'B', 3: 'H', 4: 'I', 5: 'q'}  # codes for struct.pack
        align = {3: 2, 4: 4, 5: 8}
        # XXX should int64 be unsigned as well? is unsigned ok for the others?
        print('Got %i index entries' % len(index))

        # figure out storage order
        order = []
        for key in index:
            if key.startswith('RPM Tag '):
                # TODO make this smarter
                rpmtag = int(key[8:])
            else:
                rpmtag = keymap[key]
            tagdata = index[key]
            if rpmtag in (62, 63):  # headersignatures, headerimmutable
                print('Warning: ignoring region tag in input data')
                continue
            order.append((rpmtag, key, tagdata))
        order.sort()

        # generate the store and index listing
        for rpmtag, key, tagdata in order:
            # need tag, dtype, offset, count for index
            if isinstance(tagdata, dict):
                dtype = tagdata['dtype']
                value = tagdata['value']
            elif key in self.typemap:
                dtype = self.typemap[key]
                value = tagdata
            elif isinstance(tagdata, str):
                dtype = 6
                value = tagdata
            elif isinstance(tagdata, list):
                if isinstance(tagdata[0], int):
                    # without explicit dtype, assume int32
                    dtype = 4
                    value = tagdata
                elif isinstance(tagdata[0], str):
                    # treat lists of strings as i18n string arrays
                    # (regular string arrays are less frequent)
                    dtype = 9
                    value = tagdata
            pad = 0
            if dtype == 0:
                # null
                assert value is None
                count = 1  # XXX?
                blob = b''
            elif dtype <= 5:
                # integer types
                count = len(value)
                fmt = f'!{count}{int_fmt[dtype]}'
                blob = struct.pack(fmt, *value)
                size = align.get(dtype)
                if size:
                    pad = (size - (ofs % size)) % size
                    blob = b'\00' * pad + blob
            elif dtype == 6:
                # string
                count = 1
                blob = value.encode(errors='surrogatepass') + b'\0'
            elif dtype == 7:
                blob = base64.b64decode(value)
                count = len(blob)
            elif dtype in (8, 9):
                # string array or i18n string array
                count = len(value)
                blob = b'\0'.join([v.encode(errors='surrogatepass') for v in value]) + b'\0'

            store.append(blob)

            idx.append([rpmtag, dtype, ofs + pad, count])
            ofs += len(blob)

        # add region tag
        # see also:
        #   https://rpm-software-management.github.io/rpm/manual/format_header.html#immutable-regions
        #   https://dentrassi.de/2016/04/15/writing-rpm-files-in-plain-java/
        if self.is_sig:
            rpmtag = RPM_TAG_HEADERSIGNATURES
        else:
            rpmtag = 63  # XXX
        dtype = 7
        offset = (len(idx) + 1) * 16
        # this is the offset *within the index* before which entries are immutable
        # we add 1 to the length to account for the entry we're about to add
        offset = -offset  # rpm negates this value
        count = 16
        fmt = r'!4i'  # 4 signed 4-byte ints, network order
        blob = struct.pack(fmt, rpmtag, dtype, offset, count)
        # no padding for this dtype
        # region tag goes at the beginning of the index
        idx.insert(0, [rpmtag, dtype, ofs, count])
        store.append(blob)
        ofs += len(blob)

        # make the index blob
        parts = [RPM_HEADER_MAGIC,
                 struct.pack('!B', version),
                 b'\00\00\00\00',  # reserved
                 struct.pack('!I', len(idx)),
                 struct.pack('!I', ofs)]
        for rpmtag, dtype, ofs, count in idx:
            parts.append(struct.pack('!IIII', rpmtag, dtype, ofs, count))
        parts.extend(store)

        blob = b''.join(parts)
        if self.is_sig:
            # "As of RPM 2.1 ... the Signature section is padded to a multiple of 8 bytes"
            pad = (8 - (len(blob) % 8)) % 8
            if pad:
                blob += b'\00' * pad

        return blob

    def decode_bytes(self, value):
        if six.PY2:
            return value
        else:
            return value.decode(errors='surrogateescape')

    def __getitem__(self, key):
        tag, dtype, offset, count = self.index[key]
        assert tag == key
        return self._getitem(dtype, offset, count)

    def _getitem(self, dtype, offset, count, decode=None):
        if decode is None:
            decode = self.decode
        # calculate start of store
        il = len(self._order)
        store = 16 + il * 16
        pos = store + offset
        if dtype >= 2 and dtype <= 5:
            values = []
            for _ in range(count):
                n = 1 << (dtype - 2)
                # n-byte integer
                data = [_ord(x) for x in self.header[pos:pos + n]]
                values.append(multibyte(data))
                pos += n
            return values
        elif dtype == 1:
            # char treated like int8
            return [_ord(c) for c in self.header[pos:pos + count]]
        elif dtype == 6:
            # string (null terminated)
            end = self.header.find(six.b('\0'), pos)
            value = self.header[pos:end]
            if decode:
                value = self.decode_bytes(value)
            return value
        elif dtype == 7:
            # raw data
            return self.header[pos:pos + count]
        elif dtype == 8:
            # string array
            result = []
            for _ in range(count):
                end = self.header.find(six.b('\0'), pos)
                value = self.header[pos:end]
                if decode:
                    value = self.decode_bytes(value)
                result.append(value)
                pos = end + 1
            return result
        elif dtype == 9:
            # i18n string array
            # note that we do not apply localization
            result = []
            for _ in range(count):
                end = self.header.find(six.b('\0'), pos)
                value = self.header[pos:end]
                if decode:
                    value = self.decode_bytes(value)
                result.append(value)
                pos = end + 1
            return result
        else:
            raise ValueError("Unknown header data type: %x" % dtype)

    def get(self, key, default=None, decode=None, single=False):
        # With decode on, we will _mostly_ return the same value that rpmlib will.
        # There are exceptions where rpmlib will automatically translate or update values, e.g.
        # * fields that rpm treats as scalars
        # * special tags like Headerimmutable
        # * i18n string translations
        # * the Fileclass extension tag that overlaps a concrete tag
        # * auto converting PREINPROG/POSTINPROG/etc to string arrays for older rpms
        entry = self.index.get(key)
        if entry is None:
            return default
        else:
            value = self._getitem(*entry[1:], decode=decode)
            if single and isinstance(value, list):
                if len(value) == 1:
                    return value[0]
                else:
                    raise ValueError('single value requested for array at key %s' % key)
            return value


def encode_bytes(value):
    if isinstance(value, bytes):
        return base64.b64encode(value).decode('ascii')
    else:
        return value


def encode_bytes_recurse(value):
    walker = DataWalker(value, encode_bytes)
    return walker.walk()


def rip_rpm_sighdr(src):
    """Rip the signature header out of an rpm"""
    (start, size) = find_rpm_sighdr(src)
    fo = open(src, 'rb')
    fo.seek(start, 0)
    sighdr = fo.read(size)
    fo.close()
    return sighdr


def rip_rpm_hdr(src):
    """Rip the main header out of an rpm"""
    (start, size) = find_rpm_sighdr(src)
    start += size
    size = rpm_hdr_size(src, start)
    fo = open(src, 'rb')
    fo.seek(start, 0)
    hdr = fo.read(size)
    fo.close()
    return hdr


def _ord(s):
    """Convert a string (single character) to an int (byte).

    Use this method to get bytes from RPM headers in Python 2 and 3.

    :returns: int
    """
    if isinstance(s, int):
        # In Python 3, RPM headers are bytes (sequences of integers), so we
        # already have an int here:
        return s
    else:
        # In Python 2, RPM headers are strings, so we have a one-character
        # string here, which we must convert to an int:
        return ord(s)


def hex_string(s):
    """Converts a string to a string of hex digits"""
    return ''.join(['%02x' % _ord(x) for x in s])


if __name__ == '__main__':
    import traceback
    import pdb
    try:
        main()
    except Exception:
        etype, e, tb = sys.exc_info()
        traceback.print_exc()
        pdb.post_mortem(tb)
